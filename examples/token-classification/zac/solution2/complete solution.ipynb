{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import json\n",
    "with open('train/train_split.jsonl',encoding=\"utf-8\") as f:\n",
    "    train = [json.loads(jline) for jline in f.read().splitlines()]\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['Lê_Hoàng_Anh']]\n"
     ]
    }
   ],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "rdrsegmenter = VnCoreNLP(\"/home/hoanganh/zac2020-IE/VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
    "\n",
    "# Input \n",
    "text = \"Lê Hoàng Anh\"\n",
    "\n",
    "# To perform word (and sentence) segmentation\n",
    "sentences = rdrsegmenter.tokenize(text) \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'sfad_fsad._f'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "'sfad fsad. f'.replace(' ','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_replace(s, sub, replace):\n",
    "    for i in range(len(s)-len(sub)):\n",
    "        if sub\n",
    "for item in train:\n",
    "    team1 = item['match_summary']['players']['team1']\n",
    "    for para in item['original_doc']['_source']['body']:\n",
    "        para.replace(team1,team1.replace(\" \",''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_arr(sub, arr, idx):\n",
    "    ''' Match sub in arr at index idx'''\n",
    "    for i in range(len(sub)):\n",
    "        if sub[i]!=arr[idx+i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_array(sub,arr):\n",
    "    ''' Find first appearance of sub in arr '''\n",
    "    if len(sub)==0 or len(arr)==0:\n",
    "        return -1\n",
    "    for i in range(len(arr)-len(sub)+1):\n",
    "        if match_arr(sub,arr,i):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def find_all_array(sub,arr):\n",
    "    ''' Find all appearances of sub in arr '''\n",
    "    result=[]\n",
    "    for i in range(len(arr)-len(sub)+1):\n",
    "        if match_arr(sub,arr,i):\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "import itertools\n",
    "\n",
    "def preprocess(text):\n",
    "    return text.replace('&amp;','&')\n",
    "\n",
    "def tokenize(text):\n",
    "    token_sentences = rdrsegmenter.tokenize(preprocess(text))\n",
    "    return list(itertools.chain.from_iterable(token_sentences))\n",
    "\n",
    "def item_to_doc(item):\n",
    "    doc = item['original_doc']['_source']['description']\n",
    "    for para in item['original_doc']['_source']['body']:\n",
    "        doc+=' '+para['text']\n",
    "    return doc\n",
    "\n",
    "def generate_ner_annotation(item):\n",
    "    team1, team2 = item['match_summary']['players']['team1'], item['match_summary']['players']['team2']\n",
    "    team1_token, team2_token = tokenize(team1), tokenize(team2)\n",
    "\n",
    "    doc_token = tokenize(item_to_doc(item))\n",
    "    tags = ['O']*len(doc_token)\n",
    "\n",
    "    team1_pos = find_all_array(team1_token,doc_token)\n",
    "    for pos in team1_pos:\n",
    "        tags[pos]='B'+'-'+'team_name'\n",
    "        for i in range(1,len(team1_token)):\n",
    "            tags[pos+i]='I'+'-'+'team_name'\n",
    "\n",
    "    team2_pos = find_all_array(team2_token,doc_token)\n",
    "    for pos in team2_pos:\n",
    "        tags[pos]='B'+'-'+'team_name'\n",
    "        for i in range(1,len(team2_token)):\n",
    "            tags[pos+i]='I'+'-'+'team_name'\n",
    "\n",
    "    return list(zip(doc_token,tags))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open('train/train.txt', 'w', \"utf-8\") as f:\n",
    "    for item in train:\n",
    "        ner = generate_ner_annotation(item)[:256]\n",
    "        for token in ner:\n",
    "            f.write(f'{token[0]} {token[1]}\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('train/val_split.jsonl',encoding=\"utf-8\") as f:\n",
    "    train = [json.loads(jline) for jline in f.read().splitlines()]\n",
    "\n",
    "import codecs\n",
    "with codecs.open('train/dev.txt', 'w', \"utf-8\") as f:\n",
    "    for item in train:\n",
    "        ner = generate_ner_annotation(item)[:256]\n",
    "        for token in ner:\n",
    "            f.write(f'{token[0]} {token[1]}\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,codecs\n",
    "with open('public_test/public_test.jsonl',encoding=\"utf-8\") as f:\n",
    "    test = [json.loads(jline) for jline in f.read().splitlines()]\n",
    "\n",
    "with codecs.open('public_test/test.txt', 'w', \"utf-8\") as f:\n",
    "    # json.dump(doc, modified,ensure_ascii=False,indent=4)\n",
    "    for idx, item in enumerate(test):\n",
    "        doc_token = tokenize(item_to_doc(item))\n",
    "        for i,token in enumerate(doc_token):\n",
    "            if i>=256:\n",
    "                break\n",
    "            f.write(f'{token} O\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}