{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Fine tune a NER model to predict player names, times, teams in score list, card list and substitution list"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Load training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "701"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "import json\n",
    "with open('train/train.jsonl',encoding=\"utf-8\") as f:\n",
    "    train = [json.loads(jline) for jline in f.read().splitlines()]\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_to_string(item):\n",
    "    s=''\n",
    "    for para in item['original_doc']['_source']['body']:\n",
    "        s+=para['text']+'\\n'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "270\n"
     ]
    }
   ],
   "source": [
    "max_length=0\n",
    "for item in train[:30]:\n",
    "    for para in item['original_doc']['_source']['body']:\n",
    "        max_length=max(max_length,len(para['text'].split()))\n",
    "print(max_length)"
   ]
  },
  {
   "source": [
    "## Tokenize and generate NER training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Load VNcoreNLP"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['Diego_Costa']]\n"
     ]
    }
   ],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "rdrsegmenter = VnCoreNLP(\"VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
    "\n",
    "# Input \n",
    "text = \"Diego Costa\"\n",
    "\n",
    "# To perform word (and sentence) segmentation\n",
    "sentences = rdrsegmenter.tokenize(text) \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_arr(sub, arr, idx):\n",
    "    for i in range(len(sub)):\n",
    "        if sub[i]!=arr[idx+i]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_array(sub,arr):\n",
    "    if len(sub)==0 or len(arr)==0:\n",
    "        return -1\n",
    "    for i in range(len(arr)-len(sub)+1):\n",
    "        if match_arr(sub,arr,i):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def find_all_array(sub,arr):\n",
    "    result=[]\n",
    "    for i in range(len(arr)-len(sub)+1):\n",
    "        if match_arr(sub,arr,i):\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "def ref_id_to_info(ref_id,match_summary):\n",
    "    ''' Input: event id\n",
    "        Output: information in match_summary refer to that event\n",
    "    '''\n",
    "    output=[]\n",
    "    for category in match_summary:\n",
    "        if category in {'score_list','card_list','substitution_list'}:\n",
    "            if isinstance(match_summary[category],list):\n",
    "                output+=[(category,item) for item in match_summary[category] if str(ref_id) in item['ref_event_ids']]\n",
    "            else:\n",
    "                if ref_id in match_summary[category]['ref_event_ids']:\n",
    "                    output.append((category,match_summary[category]))\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def tokenize(text):\n",
    "    token_sentences = rdrsegmenter.tokenize(text)\n",
    "    return list(itertools.chain.from_iterable(token_sentences))\n",
    "\n",
    "def generate_ner_annotation(item,idx_item):\n",
    "    ''' From item object --> ner annotation format'''\n",
    "    result=[]\n",
    "\n",
    "    for idx,html in enumerate(item[\"html_annotation\"]): \n",
    "        para_tokens = tokenize(item['original_doc']['_source']['body'][idx]['text'])\n",
    "        tags = ['O']*len(para_tokens)\n",
    "\n",
    "        soup = BeautifulSoup(html)\n",
    "        events = soup.find_all(\"span\", {\"class\": \"tag\"})\n",
    "        for e in events:\n",
    "            event_tokens = tokenize(e.text)\n",
    "            event_pos = find_array(event_tokens,para_tokens)\n",
    "            if event_pos==-1:\n",
    "                print(event_tokens,para_tokens)\n",
    "                print(idx_item, idx,'---------------------------')\n",
    "                return []\n",
    "                # print('--',event_tokens)\n",
    "                # print('++',para_tokens)\n",
    "\n",
    "            event_info_list = ref_id_to_info(e['event_id'],item['match_summary'])\n",
    "            # print(event_info_list)\n",
    "            for info in event_info_list:\n",
    "                for attr in info[1]:\n",
    "                    if attr!='ref_event_ids' and info[1][attr].strip()!='':\n",
    "                        attr_tokens = tokenize(info[1][attr])\n",
    "                        attr_pos_list = find_all_array(attr_tokens,event_tokens)\n",
    "                        # if len(attr_pos_list)==0:\n",
    "                            # print('---',info[1][attr])\n",
    "                        for attr_pos in attr_pos_list:\n",
    "                            tags[event_pos+attr_pos]='B'+'-'+info[0]+'_'+attr\n",
    "                            for i in range(1,len(attr_tokens)):\n",
    "                                tags[event_pos+attr_pos+i]='I'+'-'+info[0]+'_'+attr\n",
    "                            # print(info[1][attr],tags[event_pos+attr_pos])\n",
    "\n",
    "        # print(tags)\n",
    "        result.append(list(zip(para_tokens,tags)))\n",
    "    return result\n"
   ]
  },
  {
   "source": [
    "### Generate NER annotation file for train dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "24 9 ---------------------------\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "with codecs.open('train.txt', 'w', \"utf-8\") as f:\n",
    "    # json.dump(doc, modified,ensure_ascii=False,indent=4)\n",
    "    for idx,item in enumerate(train):\n",
    "        try:\n",
    "            ner_ann = generate_ner_annotation(item,idx)\n",
    "        except:\n",
    "            print('***********',idx)\n",
    "        if len(ner_ann)==0:\n",
    "            break\n",
    "        for para in ner_ann:\n",
    "            for token in para:\n",
    "                f.write('{} {}\\n'.format(token[0],token[1]))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "source": [
    "### Generate NER annotation file for test set (all with anotation 'O')"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,codecs\n",
    "with open('/mnt/d/zaloAI2020/zac2020/data/release/private_test/private_test/private_test.jsonl',encoding=\"utf-8\") as f:\n",
    "    test = [json.loads(jline) for jline in f.read().splitlines()]\n",
    "\n",
    "cnt=0\n",
    "write_cnt=0\n",
    "with codecs.open('/home/hoanganh/zac2020-IE/ZaloAIChallenge-2020/examples/token-classification/zac/test_private.txt', 'w', \"utf-8\") as f:\n",
    "    # json.dump(doc, modified,ensure_ascii=False,indent=4)\n",
    "    for idx, item in enumerate(test):\n",
    "        for para in item['original_doc']['_source']['body']:\n",
    "            try:\n",
    "                tokens = tokenize(para['text'])\n",
    "            except:\n",
    "                print('***********',idx)\n",
    "            if len(tokens)>0:\n",
    "                write_cnt+=1\n",
    "                for token in tokens:\n",
    "                    f.write('{} O\\n'.format(token))\n",
    "                f.write('\\n')"
   ]
  },
  {
   "source": [
    "## From NER predicted result --> generate match summary information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_output_to_para(file_name,para_file_name='test_pred_para.txt'):\n",
    "    ''' From formated ner output file, generate readable paragraph with ner tag'''\n",
    "    para_cnt=0\n",
    "    with open(para_file_name,'w',encoding='utf-8') as fw:\n",
    "        with open(file_name,encoding='utf-8') as f:\n",
    "            line = ''\n",
    "            para=''\n",
    "            ner_token=''\n",
    "            ner_type=''\n",
    "            while True:\n",
    "                line = f.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                if line.strip()=='':\n",
    "                    para_cnt+=1\n",
    "                    fw.write(para + '\\n------------\\n')\n",
    "                    para=''\n",
    "                else:\n",
    "                    token, type = line.strip().split()\n",
    "                    if type[0]=='I':\n",
    "                        ner_token+=' '+token\n",
    "                    elif type!='O':\n",
    "                        ner_type=type[2:]\n",
    "                        ner_token=token\n",
    "                    else:\n",
    "                        if ner_type!='':\n",
    "                            para+=f'<{ner_type}>({ner_token})'\n",
    "                            ner_type=ner_token=''                        \n",
    "                        para+=f' {token}'\n",
    "                \n",
    "            if ner_type!='':\n",
    "                para+=f'<{ner_type}>({ner_token})'\n",
    "\n",
    "    print(para_cnt)\n",
    "\n",
    "ner_output_to_para('/home/hoanganh/zac2020-IE/test_predictions (2).txt')          \n"
   ]
  },
  {
   "source": [
    "### Only generate goal scorers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def para_id_to_item_id(dataset):\n",
    "    para_cnt=[]\n",
    "    for item in dataset:\n",
    "        para_cnt.append(len(item['original_doc']['_source']['body']))\n",
    "    return para_cnt\n",
    "    \n",
    "def ner_pred_to_scorers(ner_file,para_cnt_list,tag='score_list_player_name'):\n",
    "    para_cnt=0\n",
    "    item_id=0\n",
    "    ner_token=''\n",
    "    scorers_list=[]\n",
    "    scorers=[]\n",
    "    with open(ner_file,encoding='utf-8') as f:\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if line.strip()=='':\n",
    "                para_cnt+=1\n",
    "                if para_cnt>=para_cnt_list[item_id]:\n",
    "                    para_cnt=0\n",
    "                    item_id+=1\n",
    "                    scorers_list.append(scorers)\n",
    "                    scorers=[]\n",
    "                continue\n",
    "            token,type=line.strip().split()\n",
    "            if type=='B-'+tag:\n",
    "                ner_token=token\n",
    "            elif type=='I-'+tag:\n",
    "                ner_token+=token\n",
    "            elif ner_token!='':\n",
    "                scorers.append(ner_token)\n",
    "                ner_token=''\n",
    "    return scorers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('public_test/public_test.jsonl',encoding=\"utf-8\") as f:\n",
    "    test = [json.loads(jline) for jline in f.read().splitlines()]\n",
    "\n",
    "test_para_cnt=para_id_to_item_id(test)\n",
    "\n",
    "scorers_list = ner_pred_to_scorers('/home/hoanganh/zac2020-IE/test_predictions (2).txt',test_para_cnt)"
   ]
  },
  {
   "source": [
    "#### Append score list to the submit file with team names and scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('submit_scorer.jsonl',encoding=\"utf-8\") as f:\n",
    "    submit = [json.loads(jline) for jline in f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'test_id': '21464024',\n",
       " 'match_summary': {'players': {'team1': 'Chelsea', 'team2': 'Arsenal'},\n",
       "  'score_board': {'score1': 3, 'score2': 1},\n",
       "  'score_list': [{'player_name': 'Marcos_Alonso', 'time': '', 'team': ''},\n",
       "   {'player_name': 'Hazard', 'time': '', 'team': ''},\n",
       "   {'player_name': 'Cesc_Fabregas', 'time': '', 'team': ''},\n",
       "   {'player_name': 'Olivier_Giroud', 'time': '', 'team': ''}],\n",
       "  'card_list': [{'player_name': '', 'time': '', 'team': ''}],\n",
       "  'substitution_list': [{'player_in': '', 'time': '', 'player_out': ''}]}}"
      ]
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "for id,item in enumerate(submit):\n",
    "    if len(scorers_list[id])>0:\n",
    "        item['match_summary']['card_list']=[]\n",
    "        for scorer in scorers_list[id]:\n",
    "            item['match_summary']['score_list'].append({'player_name': scorer, 'time': '', 'team': ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in submit:\n",
    "    for goal in item['match_summary']['card_list']:\n",
    "        goal['player_name']=goal['player_name'].replace('_',' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open('submit_scorer_card.jsonl', 'w', \"utf-8\") as f:\n",
    "    for item in submit:\n",
    "        json.dump(item,f,ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "source": [
    "### Generate match summary for score list, card list and substitution list"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ner_tokens(ner_file):\n",
    "    para_list=[]\n",
    "    para=[]\n",
    "    ner_token=''\n",
    "    ner_type='O'\n",
    "    token_id=0\n",
    "\n",
    "    with open(ner_file,encoding='utf-8') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "\n",
    "            if line.strip()=='':\n",
    "                if ner_type!='O':\n",
    "                    para.append((token_id,ner_type,ner_token))\n",
    "                para_list.append(para)\n",
    "                para=[]\n",
    "                ner_token=''\n",
    "                ner_type='O'\n",
    "                token_id=0\n",
    "                continue\n",
    "\n",
    "            token,type=line.strip().split()\n",
    "            if type!='O':\n",
    "                if type[0]=='B':\n",
    "                    ner_type=type[2:]\n",
    "                    ner_token=token.replace('_',' ')\n",
    "                    token_id+=1\n",
    "                else:\n",
    "                    ner_token+=' '+ token.replace('_',' ')\n",
    "            else:\n",
    "                if ner_type!='O':\n",
    "                    para.append((token_id,ner_type,ner_token))\n",
    "                ner_type='O'\n",
    "                token_id+=1\n",
    "    \n",
    "    return para_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_para_list = merge_ner_tokens('/home/hoanganh/zac2020-IE/test_predictions (2).txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_ner_tokens(ner_para):\n",
    "    def nearest(ner, ner_list):\n",
    "        nearest_idx=-1\n",
    "        distance=1000\n",
    "        for idx,el in enumerate(ner_list):\n",
    "            if abs(el[0]-ner[0])<distance:\n",
    "                nearest_idx=idx\n",
    "                distance=abs(el[0]-ner[0])\n",
    "        return nearest_idx\n",
    "\n",
    "    table={'card_list':[],'score_list':[],'substitution_list':[]}\n",
    "    substitute_queue=[]\n",
    "    for ner in ner_para:\n",
    "        if ner[1]=='card_list_player_name':\n",
    "            table['card_list'].append((ner[0],{\"player_name\": ner[2], \"time\": \"\", \"team\": \"\"}))\n",
    "        elif ner[1]=='score_list_player_name':\n",
    "            table['score_list'].append((ner[0],{\"player_name\": ner[2], \"time\": \"\", \"team\": \"\"}))\n",
    "        elif 'substitution_list_player' in ner[1]:\n",
    "            if len(substitute_queue)>0 and substitute_queue[0][1]!=ner[1]:\n",
    "                if ner[1]=='substitution_list_player_in':\n",
    "                    table['substitution_list'].append((ner[0],{\"player_in\": ner[2], \"time\": \"\", \"player_out\": substitute_queue.pop(0)[2]}))\n",
    "                else:\n",
    "                    table['substitution_list'].append((ner[0],{\"player_in\": substitute_queue.pop(0)[2], \"time\": \"\", \"player_out\": ner[2]}))\n",
    "            else:\n",
    "                substitute_queue.append(ner)\n",
    "    for ner in substitute_queue:\n",
    "        if ner[1]=='substitution_list_player_in':\n",
    "            table['substitution_list'].append((ner[0],{\"player_in\": ner[2], \"time\": \"\", \"player_out\": ''}))\n",
    "        else:\n",
    "            table['substitution_list'].append((ner[0],{\"player_in\": '', \"time\": \"\", \"player_out\": ner[2]}))\n",
    "\n",
    "    for ner in ner_para:\n",
    "        if ner[1]=='card_list_time':\n",
    "            idx= nearest(ner,table['card_list'])\n",
    "            if idx!=-1:\n",
    "                table['card_list'][idx][1]['time']=ner[2]\n",
    "        elif ner[1]=='card_list_team':\n",
    "            idx= nearest(ner,table['card_list'])\n",
    "            if idx!=-1:\n",
    "                table['card_list'][idx][1]['team']=ner[2]\n",
    "\n",
    "        elif ner[1]=='score_list_time':\n",
    "            idx= nearest(ner,table['score_list'])\n",
    "            if idx!=-1:\n",
    "                table['score_list'][idx][1]['time']=ner[2]\n",
    "        elif ner[1]=='score_list_team':\n",
    "            idx= nearest(ner,table['score_list'])\n",
    "            if idx!=-1:\n",
    "                table['score_list'][idx][1]['team']=ner[2]\n",
    "        \n",
    "        elif ner[1]=='substitution_list_time':\n",
    "            idx= nearest(ner,table['substitution_list'])\n",
    "            if idx!=-1:\n",
    "                table['substitution_list'][idx][1]['time']=ner[2]\n",
    "            \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('public_test/public_test.jsonl',encoding=\"utf-8\") as f:\n",
    "    test = [json.loads(jline) for jline in f.read().splitlines()]\n",
    "    \n",
    "para_cnt=[]\n",
    "for item in test:\n",
    "    para_cnt.append(len(item['original_doc']['_source']['body']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(39, {'player_name': 'Diop', 'time': '', 'team': ''})\n(14, {'player_name': 'Sergio Aguero', 'time': '', 'team': ''})\n(23, {'player_name': 'Eric Bailly', 'time': '', 'team': ''})\n(45, {'player_name': 'Koscielny', 'time': '', 'team': ''})\n(38, {'player_name': 'Eden Hazard', 'time': '', 'team': ''})\n(41, {'player_name': 'Facundo Sebastian Roncaglia', 'time': '', 'team': ''})\n(3, {'player_name': 'Digne', 'time': '', 'team': ''})\n(11, {'player_name': 'Loic Remy', 'time': '', 'team': ''})\n(8, {'player_name': 'Marco Verratti', 'time': '', 'team': ''})\n(18, {'player_name': 'Joao Cancelo', 'time': '', 'team': ''})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('/home/hoanganh/zac2020-IE/submit.jsonl',encoding=\"utf-8\") as f:\n",
    "    submit = [json.loads(jline) for jline in f.read().splitlines()]\n",
    "\n",
    "first_para_idx=0\n",
    "\n",
    "for idx, item in enumerate(submit):\n",
    "\n",
    "    for key in ['card_list','score_list','substitution_list']:\n",
    "        item['match_summary'][key]=[]\n",
    "\n",
    "    for para_idx in range(first_para_idx,first_para_idx+para_cnt[idx]):\n",
    "        table = connect_ner_tokens(ner_para_list[para_idx])\n",
    "        for key in table:\n",
    "            for ner in table[key]:\n",
    "                # if key=='card_list':\n",
    "                #     print(ner)\n",
    "                item['match_summary'][key].append(ner[1])\n",
    "    first_para_idx+=para_cnt[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open('/home/hoanganh/zac2020-IE/submit_rule_based_relation.jsonl', 'w', \"utf-8\") as f:\n",
    "    for item in submit:\n",
    "        json.dump(item,f,ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "source": [
    "## Generate private test result from NER predicted files\n",
    "Similar to above, make a seperate section to run the code easier "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,codecs\n",
    "with open('/mnt/d/zaloAI2020/zac2020/data/release/private_test/private_test/private_test.jsonl',encoding=\"utf-8\") as f:\n",
    "    test = [json.loads(jline) for jline in f.read().splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "para_cnt=[]\n",
    "for item in test:\n",
    "    para_cnt.append(len(item['original_doc']['_source']['body']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ner_tokens(ner_file):\n",
    "    para_list=[]\n",
    "    para=[]\n",
    "    ner_token=''\n",
    "    ner_type='O'\n",
    "    token_id=0\n",
    "\n",
    "    with open(ner_file,encoding='utf-8') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "\n",
    "            if line.strip()=='':\n",
    "                if ner_type!='O':\n",
    "                    para.append((token_id,ner_type,ner_token))\n",
    "                para_list.append(para)\n",
    "                para=[]\n",
    "                ner_token=''\n",
    "                ner_type='O'\n",
    "                token_id=0\n",
    "                continue\n",
    "\n",
    "            token,type=line.strip().split()\n",
    "            if type!='O':\n",
    "                if type[0]=='B':\n",
    "                    ner_type=type[2:]\n",
    "                    ner_token=token.replace('_',' ')\n",
    "                    token_id+=1\n",
    "                else:\n",
    "                    ner_token+=' '+ token.replace('_',' ')\n",
    "            else:\n",
    "                if ner_type!='O':\n",
    "                    para.append((token_id,ner_type,ner_token))\n",
    "                ner_type='O'\n",
    "                token_id+=1\n",
    "    \n",
    "    return para_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_para_list = merge_ner_tokens('/home/hoanganh/zac2020-IE/test_predictions_private.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_ner_tokens(ner_para):\n",
    "    def nearest(ner, ner_list):\n",
    "        nearest_idx=-1\n",
    "        distance=1000\n",
    "        for idx,el in enumerate(ner_list):\n",
    "            if abs(el[0]-ner[0])<distance:\n",
    "                nearest_idx=idx\n",
    "                distance=abs(el[0]-ner[0])\n",
    "        return nearest_idx\n",
    "\n",
    "    table={'card_list':[],'score_list':[],'substitution_list':[]}\n",
    "    substitute_queue=[]\n",
    "    for ner in ner_para:\n",
    "        if ner[1]=='card_list_player_name':\n",
    "            table['card_list'].append((ner[0],{\"player_name\": ner[2], \"time\": \"\", \"team\": \"\"}))\n",
    "        elif ner[1]=='score_list_player_name':\n",
    "            table['score_list'].append((ner[0],{\"player_name\": ner[2], \"time\": \"\", \"team\": \"\"}))\n",
    "        elif 'substitution_list_player' in ner[1]:\n",
    "            if len(substitute_queue)>0 and substitute_queue[0][1]!=ner[1]:\n",
    "                if ner[1]=='substitution_list_player_in':\n",
    "                    table['substitution_list'].append((ner[0],{\"player_in\": ner[2], \"time\": \"\", \"player_out\": substitute_queue.pop(0)[2]}))\n",
    "                else:\n",
    "                    table['substitution_list'].append((ner[0],{\"player_in\": substitute_queue.pop(0)[2], \"time\": \"\", \"player_out\": ner[2]}))\n",
    "            else:\n",
    "                substitute_queue.append(ner)\n",
    "    for ner in substitute_queue:\n",
    "        if ner[1]=='substitution_list_player_in':\n",
    "            table['substitution_list'].append((ner[0],{\"player_in\": ner[2], \"time\": \"\", \"player_out\": ''}))\n",
    "        else:\n",
    "            table['substitution_list'].append((ner[0],{\"player_in\": '', \"time\": \"\", \"player_out\": ner[2]}))\n",
    "\n",
    "    for ner in ner_para:\n",
    "        if ner[1]=='card_list_time':\n",
    "            idx= nearest(ner,table['card_list'])\n",
    "            if idx!=-1:\n",
    "                table['card_list'][idx][1]['time']=ner[2]\n",
    "        elif ner[1]=='card_list_team':\n",
    "            idx= nearest(ner,table['card_list'])\n",
    "            if idx!=-1:\n",
    "                table['card_list'][idx][1]['team']=ner[2]\n",
    "\n",
    "        elif ner[1]=='score_list_time':\n",
    "            idx= nearest(ner,table['score_list'])\n",
    "            if idx!=-1:\n",
    "                table['score_list'][idx][1]['time']=ner[2]\n",
    "        elif ner[1]=='score_list_team':\n",
    "            idx= nearest(ner,table['score_list'])\n",
    "            if idx!=-1:\n",
    "                table['score_list'][idx][1]['team']=ner[2]\n",
    "        \n",
    "        elif ner[1]=='substitution_list_time':\n",
    "            idx= nearest(ner,table['substitution_list'])\n",
    "            if idx!=-1:\n",
    "                table['substitution_list'][idx][1]['time']=ner[2]\n",
    "            \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(12, {'player_name': 'Sầm Ngọc Đức', 'time': '', 'team': ''})\n(39, {'player_name': 'Van Bakel', 'time': '', 'team': ''})\n(21, {'player_name': 'Jara', 'time': '', 'team': ''})\n(54, {'player_name': 'Jara', 'time': '', 'team': ''})\n(16, {'player_name': 'Choum Pisa', 'time': '', 'team': ''})\n(14, {'player_name': 'Holding', 'time': '', 'team': ''})\n(19, {'player_name': 'Shahrul', 'time': '', 'team': ''})\n(5, {'player_name': 'Modric', 'time': '', 'team': ''})\n(25, {'player_name': 'Yevhen Khacheridi', 'time': '', 'team': ''})\n(22, {'player_name': 'Joaquin Correa', 'time': '', 'team': ''})\n(13, {'player_name': 'Sergi Roberto', 'time': '', 'team': ''})\n(37, {'player_name': 'Cabral', 'time': '', 'team': ''})\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('/home/hoanganh/zac2020-IE/submit_private.jsonl',encoding=\"utf-8\") as f:\n",
    "    submit = [json.loads(jline) for jline in f.read().splitlines()]\n",
    "\n",
    "first_para_idx=0\n",
    "\n",
    "for idx, item in enumerate(submit):\n",
    "    for key in ['card_list','score_list','substitution_list']:\n",
    "        item['match_summary'][key]=[]\n",
    "\n",
    "    for para_idx in range(first_para_idx,first_para_idx+para_cnt[idx]):\n",
    "        table = connect_ner_tokens(ner_para_list[para_idx])\n",
    "        for key in table:\n",
    "            for ner in table[key]:\n",
    "                if key=='card_list':\n",
    "                    print(ner)\n",
    "                item['match_summary'][key].append(ner[1])\n",
    "    first_para_idx+=para_cnt[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open('/home/hoanganh/zac2020-IE/submit_private_rule_based_relation.jsonl', 'w', \"utf-8\") as f:\n",
    "    for item in submit:\n",
    "        json.dump(item,f,ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  }
 ]
}